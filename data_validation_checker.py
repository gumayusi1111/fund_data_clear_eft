#!/usr/bin/env python3
"""
ETFæ•°æ®éªŒè¯è„šæœ¬
1. éªŒè¯æ—¥æ›´å’Œå‘¨æ›´æ•°æ®çš„ä¸€è‡´æ€§
2. éšæœºæŠ½å–100ä¸ªETFä»£ç éªŒè¯ä¸‰ç§å¤æƒæ•°æ®çš„å¯ä¿¡åº¦
3. ç”Ÿæˆè¯¦ç»†çš„éªŒè¯æŠ¥å‘Š
"""

import os
import sys
import pandas as pd
import random
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import json

# æ·»åŠ configç›®å½•åˆ°è·¯å¾„
config_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config')
sys.path.insert(0, config_dir)

try:
    import importlib.util
    # å¯¼å…¥æ—¥å¿—é…ç½®
    logger_config_path = os.path.join(config_dir, 'logger_config.py')
    spec = importlib.util.spec_from_file_location("logger_config", logger_config_path)
    logger_config_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(logger_config_module)
    setup_logger = logger_config_module.setup_logger
    
    # è®¾ç½®éªŒè¯ä¸“ç”¨æ—¥å¿—
    logger = setup_logger("etf_validation", "general")
except ImportError as e:
    print(f"è­¦å‘Šï¼šæ— æ³•å¯¼å…¥æ—¥å¿—é…ç½®: {e}")
    logger = None

# é…ç½®è·¯å¾„
DAILY_DIR = "./ETFæ—¥æ›´"
WEEKLY_DIR = "./ETFå‘¨æ›´"
CATEGORIES = ["0_ETFæ—¥K(å‰å¤æƒ)", "0_ETFæ—¥K(åå¤æƒ)", "0_ETFæ—¥K(é™¤æƒ)"]

def log_message(message: str, level: str = "INFO"):
    """è®°å½•æ—¥å¿—æ¶ˆæ¯"""
    if logger:
        if level == "ERROR":
            logger.error(message)
        elif level == "WARNING":
            logger.warning(message)
        else:
            logger.info(message)
    
    # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] {message}")


def get_etf_codes_from_dir(directory: str, category: str) -> List[str]:
    """è·å–æŒ‡å®šç›®å½•å’Œç±»åˆ«ä¸‹çš„æ‰€æœ‰ETFä»£ç """
    category_path = os.path.join(directory, category)
    if not os.path.exists(category_path):
        return []
    
    codes = []
    for filename in os.listdir(category_path):
        if filename.endswith('.csv'):
            code = filename.replace('.csv', '')
            codes.append(code)
    
    return sorted(codes)


def load_etf_data(directory: str, category: str, code: str) -> Optional[pd.DataFrame]:
    """åŠ è½½æŒ‡å®šETFçš„æ•°æ®"""
    file_path = os.path.join(directory, category, f"{code}.csv")
    
    if not os.path.exists(file_path):
        return None
    
    try:
        df = pd.read_csv(file_path, encoding='utf-8')
        if df.empty:
            return None
        
        # ç¡®ä¿æ—¥æœŸåˆ—ä¸ºå­—ç¬¦ä¸²ç±»å‹ä¾¿äºæ¯”è¾ƒ
        df['æ—¥æœŸ'] = df['æ—¥æœŸ'].astype(str)
        return df
    except Exception as e:
        log_message(f"åŠ è½½æ•°æ®å¤±è´¥ {file_path}: {e}", "ERROR")
        return None


def compare_daily_weekly_consistency():
    """æ¯”è¾ƒæ—¥æ›´å’Œå‘¨æ›´æ•°æ®çš„ä¸€è‡´æ€§"""
    log_message("ğŸ” å¼€å§‹éªŒè¯æ—¥æ›´å’Œå‘¨æ›´æ•°æ®ä¸€è‡´æ€§...")
    
    results = {
        "total_codes": 0,
        "consistent_codes": 0,
        "inconsistent_codes": 0,
        "missing_in_daily": 0,
        "missing_in_weekly": 0,
        "inconsistent_details": []
    }
    
    # å¯¹æ¯ä¸ªç±»åˆ«è¿›è¡Œæ£€æŸ¥
    for category in CATEGORIES:
        log_message(f"ğŸ“Š æ£€æŸ¥ç±»åˆ«: {category}")
        
        # è·å–ä¸¤è¾¹çš„ETFä»£ç 
        daily_codes = set(get_etf_codes_from_dir(DAILY_DIR, category))
        weekly_codes = set(get_etf_codes_from_dir(WEEKLY_DIR, category))
        
        log_message(f"  æ—¥æ›´ETFæ•°é‡: {len(daily_codes)}")
        log_message(f"  å‘¨æ›´ETFæ•°é‡: {len(weekly_codes)}")
        
        # æ‰¾å‡ºå…±åŒçš„ETFä»£ç 
        common_codes = daily_codes & weekly_codes
        missing_in_daily = weekly_codes - daily_codes
        missing_in_weekly = daily_codes - weekly_codes
        
        log_message(f"  å…±åŒETFæ•°é‡: {len(common_codes)}")
        if missing_in_daily:
            log_message(f"  âš ï¸ æ—¥æ›´ç¼ºå¤±: {len(missing_in_daily)} ä¸ª")
        if missing_in_weekly:
            log_message(f"  âš ï¸ å‘¨æ›´ç¼ºå¤±: {len(missing_in_weekly)} ä¸ª")
        
        results["missing_in_daily"] += len(missing_in_daily)
        results["missing_in_weekly"] += len(missing_in_weekly)
        
        # éšæœºæŠ½å–ä¸€äº›ä»£ç è¿›è¡Œè¯¦ç»†æ¯”è¾ƒ
        sample_codes = random.sample(list(common_codes), min(10, len(common_codes)))
        
        for code in sample_codes:
            daily_df = load_etf_data(DAILY_DIR, category, code)
            weekly_df = load_etf_data(WEEKLY_DIR, category, code)
            
            if daily_df is None or weekly_df is None:
                continue
            
            results["total_codes"] += 1
            
            # æ¯”è¾ƒå…±åŒæ—¥æœŸçš„æ•°æ®
            daily_dates = set(daily_df['æ—¥æœŸ'])
            weekly_dates = set(weekly_df['æ—¥æœŸ'])
            common_dates = daily_dates & weekly_dates
            
            if not common_dates:
                log_message(f"  âš ï¸ {code}: æ²¡æœ‰å…±åŒæ—¥æœŸæ•°æ®", "WARNING")
                continue
            
            # å–æœ€è¿‘çš„å‡ ä¸ªæ—¥æœŸè¿›è¡Œæ¯”è¾ƒ
            recent_dates = sorted(list(common_dates))[-5:]  # æœ€è¿‘5å¤©
            
            is_consistent = True
            for date in recent_dates:
                daily_row = daily_df[daily_df['æ—¥æœŸ'] == date]
                weekly_row = weekly_df[weekly_df['æ—¥æœŸ'] == date]
                
                if daily_row.empty or weekly_row.empty:
                    continue
                
                # æ¯”è¾ƒä»·æ ¼å­—æ®µï¼ˆå…è®¸å°çš„æµ®ç‚¹è¯¯å·®ï¼‰
                price_fields = ['å¼€ç›˜ä»·', 'æœ€é«˜ä»·', 'æœ€ä½ä»·', 'æ”¶ç›˜ä»·']
                for field in price_fields:
                    if field in daily_row.columns and field in weekly_row.columns:
                        daily_val = float(daily_row[field].iloc[0])
                        weekly_val = float(weekly_row[field].iloc[0])
                        
                        # å…è®¸0.01çš„è¯¯å·®ï¼ˆ1åˆ†é’±ï¼‰
                        if abs(daily_val - weekly_val) > 0.01:
                            is_consistent = False
                            results["inconsistent_details"].append({
                                "code": code,
                                "category": category,
                                "date": date,
                                "field": field,
                                "daily_value": daily_val,
                                "weekly_value": weekly_val,
                                "difference": abs(daily_val - weekly_val)
                            })
            
            if is_consistent:
                results["consistent_codes"] += 1
            else:
                results["inconsistent_codes"] += 1
    
    return results


def validate_three_adjustments_accuracy():
    """éªŒè¯ä¸‰ç§å¤æƒæ•°æ®çš„å‡†ç¡®æ€§"""
    log_message("ğŸ”¬ å¼€å§‹éªŒè¯ä¸‰ç§å¤æƒæ•°æ®å‡†ç¡®æ€§...")
    
    # éšæœºé€‰æ‹©100ä¸ªETFä»£ç è¿›è¡ŒéªŒè¯
    all_codes = get_etf_codes_from_dir(DAILY_DIR, CATEGORIES[0])  # ä»å‰å¤æƒç›®å½•è·å–ä»£ç 
    sample_codes = random.sample(all_codes, min(100, len(all_codes)))
    
    log_message(f"ğŸ“ éšæœºæŠ½å– {len(sample_codes)} ä¸ªETFè¿›è¡ŒéªŒè¯")
    
    results = {
        "total_samples": len(sample_codes),
        "valid_samples": 0,
        "calculation_errors": 0,
        "price_anomalies": 0,
        "adjustment_accuracy": {
            "high_accuracy": 0,    # å·®å¼‚ < 0.1%
            "medium_accuracy": 0,  # å·®å¼‚ 0.1% - 1%
            "low_accuracy": 0,     # å·®å¼‚ > 1%
        },
        "details": []
    }
    
    for i, code in enumerate(sample_codes, 1):
        if i % 20 == 0:
            log_message(f"  è¿›åº¦: {i}/{len(sample_codes)}")
        
        # åŠ è½½ä¸‰ç§å¤æƒæ•°æ®
        forward_df = load_etf_data(DAILY_DIR, "0_ETFæ—¥K(å‰å¤æƒ)", code)
        backward_df = load_etf_data(DAILY_DIR, "0_ETFæ—¥K(åå¤æƒ)", code)
        no_adjust_df = load_etf_data(DAILY_DIR, "0_ETFæ—¥K(é™¤æƒ)", code)
        
        if any(df is None for df in [forward_df, backward_df, no_adjust_df]):
            continue
        
        # æ‰¾åˆ°å…±åŒæ—¥æœŸ
        forward_dates = set(forward_df['æ—¥æœŸ'])
        backward_dates = set(backward_df['æ—¥æœŸ'])
        no_adjust_dates = set(no_adjust_df['æ—¥æœŸ'])
        common_dates = forward_dates & backward_dates & no_adjust_dates
        
        if not common_dates:
            continue
        
        results["valid_samples"] += 1
        
        # éšæœºé€‰æ‹©å‡ ä¸ªæ—¥æœŸè¿›è¡ŒéªŒè¯
        test_dates = random.sample(list(common_dates), min(5, len(common_dates)))
        
        code_errors = []
        for date in test_dates:
            forward_row = forward_df[forward_df['æ—¥æœŸ'] == date].iloc[0]
            backward_row = backward_df[backward_df['æ—¥æœŸ'] == date].iloc[0]
            no_adjust_row = no_adjust_df[no_adjust_df['æ—¥æœŸ'] == date].iloc[0]
            
            # éªŒè¯å¤æƒè®¡ç®—çš„ä¸€è‡´æ€§
            # ç†è®ºä¸Šï¼šå‰å¤æƒä»· Ã— åå¤æƒä»· / é™¤æƒä»·Â² â‰ˆ 1 (å¤æƒå› å­å…³ç³»)
            try:
                forward_price = float(forward_row['æ”¶ç›˜ä»·'])
                backward_price = float(backward_row['æ”¶ç›˜ä»·'])
                no_adjust_price = float(no_adjust_row['æ”¶ç›˜ä»·'])
                
                # æ£€æŸ¥ä»·æ ¼æ˜¯å¦åˆç†ï¼ˆå¤§äº0ï¼‰
                if any(price <= 0 for price in [forward_price, backward_price, no_adjust_price]):
                    results["price_anomalies"] += 1
                    continue
                
                # è®¡ç®—å¤æƒå› å­ï¼ˆä»é™¤æƒä»·æ ¼æ¨ç®—ï¼‰
                # å‡è®¾ï¼šå‰å¤æƒ = é™¤æƒ / å¤æƒå› å­ï¼Œåå¤æƒ = é™¤æƒ Ã— å¤æƒå› å­
                # åˆ™ï¼šå‰å¤æƒ Ã— åå¤æƒ = é™¤æƒÂ²
                expected_product = no_adjust_price * no_adjust_price
                actual_product = forward_price * backward_price
                
                # è®¡ç®—è¯¯å·®ç™¾åˆ†æ¯”
                error_percentage = abs(expected_product - actual_product) / expected_product * 100
                
                # åˆ†ç±»å‡†ç¡®åº¦
                if error_percentage < 0.1:
                    accuracy_level = "high_accuracy"
                elif error_percentage < 1.0:
                    accuracy_level = "medium_accuracy"
                else:
                    accuracy_level = "low_accuracy"
                
                results["adjustment_accuracy"][accuracy_level] += 1
                
                if error_percentage > 1.0:  # è¯¯å·®è¶…è¿‡1%è®°å½•è¯¦æƒ…
                    code_errors.append({
                        "date": date,
                        "forward_price": forward_price,
                        "backward_price": backward_price,
                        "no_adjust_price": no_adjust_price,
                        "error_percentage": error_percentage
                    })
                
            except (ValueError, ZeroDivisionError) as e:
                results["calculation_errors"] += 1
                continue
        
        if code_errors:
            results["details"].append({
                "code": code,
                "errors": code_errors
            })
    
    return results


def generate_validation_report(consistency_results: dict, accuracy_results: dict):
    """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
    log_message("ğŸ“‹ ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
    
    report = {
        "validation_time": datetime.now().isoformat(),
        "consistency_check": consistency_results,
        "accuracy_check": accuracy_results,
        "summary": {}
    }
    
    # è®¡ç®—æ±‡æ€»æŒ‡æ ‡
    if consistency_results["total_codes"] > 0:
        consistency_rate = consistency_results["consistent_codes"] / consistency_results["total_codes"] * 100
    else:
        consistency_rate = 0
    
    if accuracy_results["valid_samples"] > 0:
        high_accuracy_rate = accuracy_results["adjustment_accuracy"]["high_accuracy"] / accuracy_results["valid_samples"] * 100
        total_accuracy_count = sum(accuracy_results["adjustment_accuracy"].values())
        overall_accuracy_rate = total_accuracy_count / accuracy_results["valid_samples"] * 100 if accuracy_results["valid_samples"] > 0 else 0
    else:
        high_accuracy_rate = 0
        overall_accuracy_rate = 0
    
    report["summary"] = {
        "data_consistency_rate": round(consistency_rate, 2),
        "high_accuracy_rate": round(high_accuracy_rate, 2),
        "overall_accuracy_rate": round(overall_accuracy_rate, 2),
        "total_codes_checked": consistency_results["total_codes"],
        "total_samples_validated": accuracy_results["valid_samples"]
    }
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = f"validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    log_message(f"ğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    return report


def print_summary_report(report: dict):
    """æ‰“å°æ±‡æ€»æŠ¥å‘Š"""
    log_message("=" * 60)
    log_message("ğŸ“Š ETFæ•°æ®éªŒè¯æ±‡æ€»æŠ¥å‘Š")
    log_message("=" * 60)
    
    summary = report["summary"]
    consistency = report["consistency_check"]
    accuracy = report["accuracy_check"]
    
    # ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ
    log_message("ğŸ” æ—¥æ›´ä¸å‘¨æ›´æ•°æ®ä¸€è‡´æ€§:")
    log_message(f"  æ£€æŸ¥ä»£ç æ•°é‡: {consistency['total_codes']}")
    log_message(f"  ä¸€è‡´ä»£ç æ•°é‡: {consistency['consistent_codes']}")
    log_message(f"  ä¸ä¸€è‡´ä»£ç æ•°é‡: {consistency['inconsistent_codes']}")
    log_message(f"  ä¸€è‡´æ€§æ¯”ä¾‹: {summary['data_consistency_rate']}%")
    
    if consistency['missing_in_daily'] > 0:
        log_message(f"  âš ï¸ æ—¥æ›´ç¼ºå¤±æ–‡ä»¶: {consistency['missing_in_daily']}")
    if consistency['missing_in_weekly'] > 0:
        log_message(f"  âš ï¸ å‘¨æ›´ç¼ºå¤±æ–‡ä»¶: {consistency['missing_in_weekly']}")
    
    log_message("")
    
    # å¤æƒå‡†ç¡®æ€§æ£€æŸ¥ç»“æœ
    log_message("ğŸ”¬ ä¸‰ç§å¤æƒæ•°æ®å‡†ç¡®æ€§:")
    log_message(f"  æŠ½æ ·ä»£ç æ•°é‡: {accuracy['total_samples']}")
    log_message(f"  æœ‰æ•ˆæ ·æœ¬æ•°é‡: {accuracy['valid_samples']}")
    log_message(f"  é«˜ç²¾åº¦æ ·æœ¬: {accuracy['adjustment_accuracy']['high_accuracy']} (è¯¯å·®<0.1%)")
    log_message(f"  ä¸­ç­‰ç²¾åº¦æ ·æœ¬: {accuracy['adjustment_accuracy']['medium_accuracy']} (è¯¯å·®0.1%-1%)")
    log_message(f"  ä½ç²¾åº¦æ ·æœ¬: {accuracy['adjustment_accuracy']['low_accuracy']} (è¯¯å·®>1%)")
    log_message(f"  é«˜ç²¾åº¦æ¯”ä¾‹: {summary['high_accuracy_rate']}%")
    log_message(f"  æ•´ä½“å¯ä¿¡åº¦: {summary['overall_accuracy_rate']}%")
    
    if accuracy['price_anomalies'] > 0:
        log_message(f"  âš ï¸ ä»·æ ¼å¼‚å¸¸: {accuracy['price_anomalies']} ä¸ªæ ·æœ¬")
    if accuracy['calculation_errors'] > 0:
        log_message(f"  âš ï¸ è®¡ç®—é”™è¯¯: {accuracy['calculation_errors']} ä¸ªæ ·æœ¬")
    
    log_message("")
    
    # æ€»ä½“è¯„ä¼°
    log_message("ğŸ æ€»ä½“è¯„ä¼°:")
    if summary['data_consistency_rate'] >= 95 and summary['high_accuracy_rate'] >= 90:
        log_message("  âœ… æ•°æ®è´¨é‡ä¼˜ç§€ï¼šä¸€è‡´æ€§å’Œå‡†ç¡®æ€§éƒ½å¾ˆé«˜")
    elif summary['data_consistency_rate'] >= 90 and summary['high_accuracy_rate'] >= 80:
        log_message("  âœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼šåŸºæœ¬æ»¡è¶³ä½¿ç”¨è¦æ±‚")
    elif summary['data_consistency_rate'] >= 80 or summary['high_accuracy_rate'] >= 70:
        log_message("  âš ï¸ æ•°æ®è´¨é‡ä¸€èˆ¬ï¼šéœ€è¦æ³¨æ„éƒ¨åˆ†å¼‚å¸¸æ•°æ®")
    else:
        log_message("  âŒ æ•°æ®è´¨é‡è¾ƒå·®ï¼šå»ºè®®æ£€æŸ¥æ•°æ®å¤„ç†æµç¨‹")
    
    log_message("=" * 60)


def main():
    """ä¸»å‡½æ•°"""
    log_message("ğŸš€ ETFæ•°æ®éªŒè¯ç¨‹åºå¯åŠ¨")
    log_message(f"â° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(DAILY_DIR):
        log_message(f"âŒ æ—¥æ›´ç›®å½•ä¸å­˜åœ¨: {DAILY_DIR}", "ERROR")
        return False
    
    if not os.path.exists(WEEKLY_DIR):
        log_message(f"âŒ å‘¨æ›´ç›®å½•ä¸å­˜åœ¨: {WEEKLY_DIR}", "ERROR")
        return False
    
    try:
        # 1. éªŒè¯æ—¥æ›´å’Œå‘¨æ›´æ•°æ®ä¸€è‡´æ€§
        consistency_results = compare_daily_weekly_consistency()
        
        log_message("")
        
        # 2. éªŒè¯ä¸‰ç§å¤æƒæ•°æ®å‡†ç¡®æ€§
        accuracy_results = validate_three_adjustments_accuracy()
        
        log_message("")
        
        # 3. ç”ŸæˆéªŒè¯æŠ¥å‘Š
        report = generate_validation_report(consistency_results, accuracy_results)
        
        # 4. æ‰“å°æ±‡æ€»æŠ¥å‘Š
        print_summary_report(report)
        
        log_message(f"â° å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        log_message("ğŸ‰ æ•°æ®éªŒè¯å®Œæˆï¼")
        
        return True
        
    except Exception as e:
        log_message(f"âŒ éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}", "ERROR")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 